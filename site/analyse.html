<!doctype html>
<html>
<head>
	<meta charset="UTF-8" />
	<title>PROJET fast-food</title>	
	<link rel="stylesheet" href="style.css" />
</head>

<!-- tête -->
<body>
	<div id="page1">
		<div id="page2">
			<div id="header">
				<h1>Fast-food</h1>
				<div class="description">Fast-food - Restauration rapide - Fast food - Street food - 快餐 - 便餐 - 速食 </div>
			</div>
				<div id="menulinks">
					<a href="../index.html">
						<span>Acceuil</span>
					</a>
					<a href="presentation.html">
						<span>Présentation</span>
					</a>
					<a href="scripts.html">
						<span>Scripts</span>
					</a>
					<a href="tableaux.html">
						<span>Tableaux</span>
					</a>
					<a  class="active" href="analyse.html">
						<span>Analyse</span>
					</a>
					<a href="https://github.com/KenzaAHMIA/PROJET-FASTFOOD">
						<span>Github</span>
					</a>
				</div>
<!--------------->

<!-- Contenu ---->
				<div id="mainarea">
					<div id="contentarea">
					<br>
						<h1 class="titre">Analyse</h1>
						<p>
							Avant de procéder à l'analyse de nos données, nous avons effectué une étape crutiel qui est la normalisation du contenu.
						</p>
						<p>
							Cette étape permet d'avoir un corpus propre. 
							Elle consiste à éliminer un ou plusieurs éléments du texte comme : les mots vides (stop words), les balises, certaines ponctuations, ou des mots que l'on juge brouilleurs de résultats.
						</p>
						<p>
							Pour le français et l'anglais nous avons effectué la normalistion manuellement c'est à dire en supprimant des éléments directement ou avec recherche et remplaçe (fonctionnalisé intégré dans l'éditeur de texte).
						</p>
						<p>
							Particuliérement pour le français nous avons utilisé la commande <i>sed</i> pour remplacer les numérotations par des espaces car elles étaients trop nombreuses.
						</p>
						<p>
							Pour le chinois c'était différents car il fallait d'abord ségmenter le contenu.
							Pour cela, nous nous sommes servis du script <b>segment.sh</b> rédigé par Stanford. 
							Le dossier '<i>stanford-segmenter-2020-11-17</i>' étant trop lourd n'a pas pu être ajouté sur le Git. 
							Veuillez le trouver <a href="https://nlp.stanford.edu/software/segmenter.shtml".>ici</a>.
						</p>
							<h2>Itrameur</h2>
								<p><a href="http://www.tal.univ-paris3.fr/trameur/iTrameur/" >Itrameur</a> est une application en ligne de textométrie développé par <b>Serges Fleury</b> (maître de conférences en linguistique informatique).</p>
								<h3> Français </h3>
									<div id="itrameur"></div>
								
								<h3> Anglais </h3>
									<div id="itrameur"></div>

									</p>
								<h3> Chinois </h3>
									<p> </p>
									<p>Notre première analyse porte sur les segments répétés dont la fréquence est bien élevée dans notre corpus. Sur le graphique, le mot « 品牌 » (la marque) est le plus fréquent et nous voyons aussi « 连锁店 » (restaurant à succursales) parmi les segments répétés. Cela ne nous étonne pas parce qu’une grande partie de notre corpus provient du site d’information qui en parle souvent beaucoup.</p>
									</p>
							<h2>Nuages de mots</h2>
							<h3> Français </h3>
								<div id="nuagesmots">
								<img class="nuages" src="../PROJET/analyse/fr_nuage_mot.png" width="auto" height="300">
							</div>

							<h3> Anglais </h3>
							<div id="nuagesmots">
								<img class="nuages" src="../PROJET/analyse/en_nuage_mot.png">
							</div>
							
							<h3> Chinois </h3>
							<div id="nuagesmots">
								<img class="nuages" src="../PROJET/analyse/ch_nuage_mot.png">
							</div>



					</div>

					<div id="sidebar">
						
					</div>
				</div>


<!--------------->

<!--pied de page-->
<br>
				<div id = retour >
						<a href="#top">
							<img src="./images/pic.jpg" alt="Retourner en haut">
						</a>
				</div>

				<div id="footer">
					<p>&copy; AU 2022/2023 - PPE : Projet FAST-FOOD - K.AHMIA , Y.XUAN, S.GLAM 
					</p>
				</div>
<!--------------->

		</div>
	</div>
</body>
</html>